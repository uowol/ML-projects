{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n",
    "# Non-Null Count 체크하고 결측치 유무를 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "EDA(Exploratory Data Analysis, 탐색적 자료 분석)란, 수집한 데이터를 분석하기 전에 데이터의 특성을 관찰하고 이해하는 단계입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫번째, Target(y) 데이터에 대한 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 warning 무시\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 한글 폰트를 사용하기 위한 코드\n",
    "fe = fm.FontEntry(fname = 'NotoSansKR-Regular.otf', name = 'NotoSansKR')\n",
    "fm.fontManager.ttflist.insert(0, fe)\n",
    "plt.rc('font', family='NotoSansKR')\n",
    "\n",
    "## 코드 작성          \n",
    "train['next_arrive_time'].plot(figsize=(20,10), alpha=0.4)\n",
    "\n",
    "plt.title('버스 운행시간')\n",
    "plt.xlabel('인덱스')\n",
    "plt.ylabel('운행시간')\n",
    "\n",
    "plt.hlines([20, 250,600,2300], xmin=0, xmax=len(train), color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구간별 횟수 시각화\n",
    "\n",
    "a = train[train['next_arrive_time']<20]\n",
    "b = train[(train['next_arrive_time']>=20) & (train['next_arrive_time']<250)]\n",
    "c = train[(train['next_arrive_time']>=250) & (train['next_arrive_time']<600)]\n",
    "d = train[(train['next_arrive_time']>=600) & (train['next_arrive_time']<2300)]\n",
    "e = train[(train['next_arrive_time']>=2300)]\n",
    "\n",
    "plt.bar(x=['a', 'b', 'c', 'd', 'e'], height=[len(a), len(b), len(c), len(d), len(e)])   # b 구간이 가장 많게 나타남\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b구간이 전체에서 차지하는 비율\n",
    "\n",
    "x = ['b구간', '나머지']\n",
    "y = [len(b)/len(train), (1-len(b)/len(train))]\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.title('버스 운행시간 구간 비율')\n",
    "\n",
    "plt.pie(y, labels=x)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "x = train['distance']\n",
    "y = train['next_arrive_time']\n",
    "\n",
    "plt.figure(dpi = 150)\n",
    "\n",
    "plt.title('거리 vs 운행시간')\n",
    "plt.xlabel('거리')\n",
    "plt.ylabel('운행시간')\n",
    "\n",
    "plt.scatter(x,y, alpha = 0.3)\n",
    "\n",
    "# 이상치 표시하기 - 직사각형\n",
    "plt.gca().add_patch(\n",
    "    patches.Rectangle(\n",
    "        (0, 600),\n",
    "        1000, 2600,               \n",
    "        edgecolor = 'deeppink',\n",
    "        fill=False,\n",
    "    ))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 두번째, Features(X) 데이터에 대한 EDA\n",
    "\n",
    "어떤 컬럼에 대해서 하나의 고유한 값만 존재한다면, 그 컬럼은 식별 가능한 컬럼이라고 할 수 있습니다.\n",
    "\n",
    "운행 정보(route_id, vh_id, route_nm) 컬럼 중 식별 가능한 컬럼이 존재한다면, 나머지는 분석에 사용하지 않아도 됩니다.\n",
    "\n",
    "그럼 이러한 칼럼이 있는지 임의로 칼럼을 뽑아 살펴볼까요?\n",
    "\n",
    "## route_id -> route_nm , route_nm -> route_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[train['route_id'] == 405136001]['route_nm'].unique())\n",
    "print(train[train['route_nm'] == '360-1']['route_id'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1:1 대응 관계였던 route_id, route_nm과 달리 vh_id는 1:n, 일대다 관계입니다.\n",
    "\n",
    "그렇다면 반대로도 실험해 볼까요?\n",
    "\n",
    "vh_id를 기준으로 하면 route_id와 route_nm의 고윳값을 구할 수 있네요.\n",
    "\n",
    "이는 vh_id가 route_id와 route_nm보다 더 많은 정보를 포함하고 있음과 동시에 식별 가능한 컬럼임을 의미합니다.\n",
    "\n",
    "    즉, vh_id를 알면 route_id, route_nm을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(text:str):\n",
    "    if (len(train[train['vh_id'] == text]['route_id'].unique()) != 1) | (len(train[train['vh_id'] == text]['route_nm'].unique()) != 1):\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "temp = list(map(check, train['vh_id'].unique()))\n",
    "set(temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 처리\n",
    "\n",
    "앞 단계에서 확인한 데이터의 특징을 바탕으로 전처리 작업을 진행해 보도록 합시다!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "1. 값에 숫자 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = list(train['route_nm'].unique()) + list(train['now_station'].unique()) + list(train['next_station'].unique())\n",
    "my_dict = {text : i for i, text in enumerate(my_list)}\n",
    "my_dict\n",
    "\n",
    "def transform_df(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    df[['route_nm', 'now_station', 'next_station']] = df[['route_nm', 'now_station', 'next_station']].applymap(lambda x: my_dict[x])\n",
    "    df['now_arrive_time'] = df['now_arrive_time'].map(lambda x: int(x[:2]))\n",
    "    return df\n",
    "\n",
    "train = transform_df(train)\n",
    "test = transform_df(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 이상치 제거\n",
    "3. 데이터 분리 (train -> train/validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.1, shuffle=False)\n",
    "\n",
    "#데이터 shape 확인\n",
    "print(f\"X_train.shape : {X_train.shape}\")\n",
    "print(f\"y_train.shape : {y_train.shape}\")\n",
    "print(f\"X_valid.shape : {X_valid.shape}\")\n",
    "print(f\"y_valid.shape : {y_valid.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "\n",
    "## Random Forest\n",
    "랜덤 포레스트는 한마디로, 훈련 과정에서 만들어진 다수의 의사 결정 나무로부터 분류된 결과를 집계해 최종적으로 분류된 데이터, 또는 평균 예측치를 출력하는 모델입니다.\n",
    "\n",
    "랜덤 포레스트에서는 이 앙상블 기법 중에서도 배깅을 이용합니다!\n",
    "\n",
    "### 배깅(Bagging)\n",
    "배깅(Bagging)은 'Bootstrap + Aggregating'의 합성어인데요.\n",
    "\n",
    "여기서 부트스트랩(Bootstrap)이란, 표본 분포를 구하기 위해 데이터를 여러 번 복원 추출(랜덤 샘플링)하는 방법입니다.\n",
    "이 때, 중복을 허용하기 때문에 단일 데이터가 여러 번 선택될 수도 있습니다.\n",
    "\n",
    "배깅은 이러한 부트스트랩을 통해서 다양한 데이터셋을 만들고, 이를 학습시킨 모델을 모으는(Arregating) 방법입니다.\n",
    "\n",
    "즉, 랜덤 포레스트에서 배깅은 모든 의사 결정 나무가 학습 데이터 세트에서 임의로 하위 데이터 세트를 추출하는 과정을 말하는 것이라 이해해 주시면 됩니다.\n",
    "\n",
    "예를 들어 학습 데이터 세트에 총 1000개의 행이 있다고 하면, 임의로 행을 100개씩 선택해서 의사 결정 나무를 만드는 것입니다.\n",
    "\n",
    "### 배깅 속성 (Bagging Feature)\n",
    "의사 결정 나무를 만들 때는 사용될 속성(feature)들을 제한하여 각 나무들에 다양성을 줘야 합니다.\n",
    "\n",
    "따라서 모든 속성(feature)들에서 임의로 일부를 선택하고, 그중 정보 획득량이 가장 높은 것을 기준으로 데이터를 분할합니다.\n",
    "\n",
    "만약 데이터 세트에 n개의 속성이 있는 경우, n 제곱근 개수만큼 무작위로 선택하는 것이 일반적입니다. (A rule of thumb)\n",
    "\n",
    "예를 들어 총 25개의 속성이 있으면 그중에서 n 제곱근인 5개의 속성만 뽑아서 살펴본 후, 정보 획득량이 가장 높은 걸 기준으로 데이터를 분할하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "# 학습된 모델을 이용해 결괏값 예측 후 상위 10개의 값 확인\n",
    "predict = model.predict(test[features])\n",
    "print('----------------------예측된 데이터의 상위 10개의 값 확인--------------------\\n')\n",
    "print(predict[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터란?\n",
    "\n",
    "하이퍼 파라미터(hyper parameter)는 모델링할 때 사용자가 직접 세팅해 주는 값을 뜻합니다.\n",
    "\n",
    "머신러닝 모델을 쓸 때 사용자가 직접 세팅해야 하는 값이 상당히 많은데, 그 모든 값이 다 하이퍼 파라미터입니다.\n",
    "\n",
    "ex)\n",
    "\n",
    "- batch_size: 배치 크기\n",
    "- (training) epochs: 반복 학습 횟수\n",
    "- optimizer: 옵티마이저\n",
    "- learning rate: 학습률\n",
    "- activation functions: 활성화 함수\n",
    "\n",
    "| 하이퍼 파라미터는 정해진 최적 값이 존재하지 않으며, 설정에 따라 성능에 큰 차이를 보이기도 합니다.\n",
    "\n",
    "| 하이퍼 파라미터와 혼용되곤 하는 **파라미터(parameter, 매개변수)** 는 학습 과정에서 생성되는 변수를 말합니다. 다시 말해서 사용자가 임의로 설정하는 값이 아닙니다.\n",
    "\n",
    "사용자가 직접 설정하면 하이퍼 파라미터, 모델 혹은 데이터에 의해 결정되면 파라미터입니다.\n",
    "\n",
    "그럼, RandomForestRegressor 모델의 하이퍼 파라미터를 튜닝하여 성능을 높여 봅시다!\n",
    "\n",
    "n_estimators : 결정 트리의 개수(defalut=100)\n",
    "criterion : 분할된 것(split)의 품질을 측정하는 기능\n",
    "max_depth : 트리의 최대 깊이\n",
    "min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수\n",
    "이외에도 랜덤 포레스트 모델에는 다양한 하이퍼 파라미터가 존재합니다.\n",
    "\n",
    "이번 하이퍼 파라미터 튜닝에서는 n_estimators와 criterion을 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, criterion='mse', random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가\n",
    "이제 모델을 이용해 예측한 결괏값과 실제값을 비교하여 모델을 평가하고 성능을 확인해 볼까요?\n",
    "\n",
    "평가산식 : RMSE\n",
    "이번 버스 운행 시간 예측 프로젝트에서 사용할 평가산식은 RMSE(Root Mean Squared Error)입니다.\n",
    "\n",
    "Image\n",
    "RMSE는 오류 지표를 실제값과 유사한 단위로 다시 변환하여 해석을 쉽게 하며, MAE보다 특이치에 강합니다.\n",
    "\n",
    "예측하고자 하는 변수, 즉 target이 수치형인 회귀 문제에 사용됩니다.\n",
    "\n",
    "<회귀 모델을 평가하는 평가 지표>\n",
    "\n",
    "- MAE(Mean Absolute Error): 모델의 예측값과 실제값 차이의 절대값의 평균\n",
    "- MSE(Mean Squared Error): 모델의 예측값과 실제값 차이의 제곱의 평균\n",
    "- RMSE(Root Mean Squared Error): MSE에 루트를 씌운 것\n",
    "\n",
    "RMSE 점수도 sklearn 패키지를 이용해 구할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "XGBoost는 Extreme Gradient Boosting의 약자입니다.\n",
    "\n",
    "이 XGBoost라는 모델을 알기 위해선, 먼저 Boosting(부스팅)에 관해 이해하실 필요가 있습니다!\n",
    "\n",
    "### Boosting\n",
    "\n",
    "Boosting은 한마디로 말해, 순차적으로 모델의 정확도를 높이는 방법입니다.\n",
    "\n",
    "Boosting에서는 먼저 전체 학습 데이터에서 일부를 선택한 하위 데이터 세트와 이를 학습할 첫 번째 모델을 만듭니다.\n",
    "\n",
    "그리고 첫 번째 모델이 잘 학습하지 못한 부분을 반영해서 두 번째 데이터 세트와 모델을 만들고,\n",
    "이런 과정을 반복해서 점진적으로 모델의 정확도를 높입니다.\n",
    "\n",
    "이러한 Boosting 기법을 이용하여 구현한 알고리즘은 Gradient Boost가 대표적인데요.\n",
    "이 Gradient Boost 알고리즘을 병렬 학습이 지원되도록 구현한 것이 바로 XGBoost 입니다.\n",
    "\n",
    "Regression, Classification 문제를 모두 지원하며, 성능과 자원 효율이 좋아서 자주 사용되는 알고리즘이라고 할 수 있습니다.\n",
    "\n",
    "### 하이퍼 파라미터\n",
    "- objective: 목적함수\n",
    "    * 'reg:squarederror'는 오차 제곱입니다.\n",
    "- n_estimators: 트리 수\n",
    "- tree_method: gpu 사용\n",
    "- eval_set: 성능 평가를 수행할 데이터 세트\n",
    "- eval_metric: 조기 종료를 위한 평가 지표\n",
    "- early_stopping_rounds: 조기 종료 조건, 평가 지표가 향상될 수 있는 반복 횟수\n",
    "- verbose: 학습 결과 출력 조건\n",
    "\n",
    "이외에도 XGBRegressor 모델에는 다양한 하이퍼 파라미터가 존재합니다.\n",
    "\n",
    "※ Early Stopping (조기 중단) 기능\n",
    "\n",
    "    GBM의 경우 n_estimators에 지정된 횟수만큼 학습을 끝까지 수행하지만, XGB의 경우 오류가 더 이상 개선되지 않으면 수행을 중지합니다. 만약 n_estimators 를 200으로 설정하고, 조기 중단 파라미터 값을 50으로 설정하면, 1부터 200회까지 부스팅을 하다가 50회를 반복하는 동안 학습 오류가 감소하지 않으면 더 이상 부스팅을 진행하지 않고 종료합니다. (ex. 100회에서 학습 오류 값이 0.8인데 101~150회 반복하는 동안 예측 오류가 0.8보다 작은 값이 하나도 없으면 부스팅을 종료) 조기 중단 기능은 불필요한 학습 시간을 단축시켜 준다는 장점이 있습니다. 하지만 이 조기 중단 값을 급격하게 줄이게 되면 모델 성능이 향상될 여지가 있음에도 불구하고 학습이 조기 중단되는 경우가 발생할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# 1. 모델 정의\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators = 3000)\n",
    "\n",
    "# 2. 모델 학습\n",
    "model.fit(X_train,y_train, eval_set=[(X_valid,y_valid)],\n",
    "          eval_metric = 'rmse',\n",
    "          early_stopping_rounds=10,\n",
    "          verbose= True\n",
    "          )\n",
    "\n",
    "# 3. 예측\n",
    "# predict() 메소드 이용\n",
    "predict = model.predict(test[features])\n",
    "\n",
    "\n",
    "# 예측값 시각화\n",
    "plt.plot(predict)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e13486f50ad8491adee3900c65648710623e910042c80717865a7ec3732c806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
